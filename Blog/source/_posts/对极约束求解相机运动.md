---
title: ch7_对极约束求解相机运动
tags: [SLAM实践]
date: {{ date }}
comments: true
mathjax: true
categories: SLAM十四讲
---

{%cq%}

我们从两张图像中，可以得到一对配对好的特征点。如果我们有若干对这样的匹配点，就可以通过这些二维图像点的对应关系，恢复出在两帧之间摄像机的运动。整个推导结果，使用好的匹配点得到本质矩阵$E$或者基础矩阵$F$抑或单应矩阵$H$，然后通过将他们分解成$R,t$，就可以推算相机的位姿。

{%endcq%}

<!-- more -->

![image-20220325110120640](image-20220325110120640.png)

设 $P$ 的空间位置为：
$$
P=[X,Y,Z]^{T}
$$
由于像素坐标与世界坐标的关系为：
$$
Z \left(
\begin{matrix}
u\\ v\\ 1
\end{matrix}
\right)=
\left(
\begin{matrix}
f_x &0 &c_x\\
0   &f_y &c_y\\
0 &0 &1\\
\end{matrix}
\right)
\left(
\begin{matrix}
X\\ Y\\ Z
\end{matrix}
\right)=KP
$$
两个像素点 $p_1 , p_2 $的像素位置为:
$$
s_1p_1=KP，s_2p_2=K(RP+t)
$$
使用齐次坐标：
$$
p_1=KP，p_2=K(RP+t)
$$
取：$x_1=K^{-1}p_1,x_2=K^{-1}p_2$,这里的$ x_1 , x_2 $是两个像素点的归一化平面上的坐标。

代入上式:
$$
x_2=Rx_1+t
$$
两边同时左乘$t^{\land}$:
$$
t^{\land}x_2=t^{\land}Rx_1
$$
两侧同时左乘$x_2^{T}$，由$t^{\land}x_2$表示$t$与$x_2$的外积，外积的定义说明$t^{\land}x_2$与$t$、$x_2$都是垂直的，故而：
$$
x_2^{T}t^{\land}x_2=x_2^{T}t^{\land}Rx_1=0\\
x_2^{T}Ex_1=0
$$
带入$p_1,p_2$:
$$
p_2^{T}K^{-T}t^{\land}RK^{-1}p_1=0\\
p_2^{T}Fp_1=0
$$
因此验证对极约束：
$$
x_2^{T}t^{\land}Rx_1=0
$$

# 对极约束求解相机运动

## 1.特征匹配

这一部分的实践是在[特征提取与匹配](https://lukeyalvin.top/2022/03/24/特征提取/#more)的基础上进行的，和之前的特征匹配没有区别，只是这里把它封装成一个函数

```cpp
// 寻找特征匹配
void find_feature_matches(
    const Mat &img_1,
    const Mat &img_2,
    std::vector<KeyPoint> &keypoints_1,
    std::vector<KeyPoint> &keypoints_2,
    std::vector<DMatch> &good_matches)
{
    //-- 初始化
    Mat descriptors_1, descriptors_2;                    // 定义描述子
    Ptr<FeatureDetector> detector = ORB::create();       //创建ORB特征检测器
    Ptr<DescriptorExtractor> descriptor = ORB::create(); //创建ORB特征描述子提取.
    Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create("BruteForce-Hamming");
    //-- 第一步:检测 Oriented FAST 角点位置
    detector->detect(img_1, keypoints_1);
    detector->detect(img_2, keypoints_2);
    //-- 第二步:根据角点位置计算 BRIEF 描述子
    descriptor->compute(img_1, keypoints_1, descriptors_1);
    descriptor->compute(img_2, keypoints_2, descriptors_2);
    //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离
    vector<DMatch> matches;
    matcher->match(descriptors_1, descriptors_2, matches);
    double min_dist = 10000, max_dist = 0;
    for (int i = 0; i < descriptors_1.rows; i++)
    {
        double dist = matches[i].distance;
        if (dist < min_dist)
            min_dist = dist;
        if (dist > max_dist)
            max_dist = dist;
    }
    //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.
    for (int i = 0; i < descriptors_1.rows; i++)
    {
        if (matches[i].distance <= max(2 * min_dist, 30.0))
        {
            good_matches.push_back(matches[i]);
        }
    }
    // Mat img_goodmatch;
    // drawMatches(img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch);
    // imshow("优化后匹配点对", img_goodmatch);

    waitKey(0);
}
```

## 2.估计两张图像间运动

当我们确定两帧图像之间的特征点以及匹配，我们就可以根据良好的匹配点来估计相机的运动，这里也是封装成了一个函数。可以使用本质矩阵、基础矩阵抑或单应矩阵，它们都是封装好的函数，传参即可，不需要个人写实现。

```cpp
// 相机位姿估计
void pose_estimation_2d2d(
    std::vector<KeyPoint> &keypoints_1,
    std::vector<KeyPoint> &keypoints_2,
    std::vector<DMatch> &good_matches,
    Mat &R, Mat &t)
{
    // 相机内参,TUM Freiburg2
    Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);

    //-- 把匹配点转换为vector<Point2f>的形式
    vector<Point2f> points1;
    vector<Point2f> points2;
    for (int i = 0; i < (int)good_matches.size(); i++)
    {
        //`queryIdx` 代表的特征点序列是 `keypoints1` 中的，
        //`trainIdx `代表的特征点序列是` keypoints2 `中的，此时这两张图中的特征点相互匹配。
        points1.push_back(keypoints_1[good_matches[i].queryIdx].pt);
        points2.push_back(keypoints_2[good_matches[i].trainIdx].pt);
    }
    //-- 计算基础矩阵
    Mat fundamental_matrix;
    fundamental_matrix = findFundamentalMat(points1, points2, CV_FM_8POINT); // 八点法求基础矩阵
    cout << "基础矩阵F：\n"
         << fundamental_matrix << endl;

    //-- 计算本质矩阵
    Point2d principal_point(325.1, 249.7); //相机光心(cx cy), TUM dataset标定值
    double focal_length = 521;             //相机焦距(fx fy), TUM dataset标定值
    Mat essential_matrix = findEssentialMat(points1, points2, focal_length, principal_point);
    cout << "本质矩阵E：\n"
         << essential_matrix << endl;

    //-- 计算单应矩阵
    // RANSAC表示基于RANSAC的鲁棒算法
    // ransacReprojThreshold将点对视为内点的最大允许重投影错误阈值（仅用于RANSAC和RHO方法）一般1-10
    Mat homography_matrix = findHomography(points1, points2, RANSAC, 3);
    cout << "单应矩阵H：\n"
         << homography_matrix << endl;

    //-- 从本质矩阵中恢复旋转和平移信息.
    recoverPose(essential_matrix, points1, points2, R, t, focal_length, principal_point);
    cout << "R: \n"
         << R << endl;
    cout << "t: \n"
         << t << endl;
}
```

## 3.验证对极约束

上面可知：
$$
x_2^{T}t^{\land}Rx_1=0
$$
首先，要知道像素坐标转相机归一化坐标的方法：

```cpp
Point2d pixel2cam(const Point2d &p, const Mat &K)
{
    // 归一化坐标Pc(X/Z,Y/Z,1)
    return Point2d(
        (p.x - K.at<double>(0, 2)) / K.at<double>(0, 0), // (u-cx)/fx = X/Z
        (p.y - K.at<double>(1, 2)) / K.at<double>(1, 1)  // (v-cy)/fy = Y/Z
    );
}
```

因此可以获得两个像素点的归一化坐标$(\frac{X}{Z},\frac{Y}{Z},1)$

实现：

```cpp
Point2d pt1 = pixel2cam(keypoints_1[m.queryIdx].pt, K); // keypoints_1[m.queryIdx].pt为像素坐标，K为相机内参矩阵
Mat y1 = (Mat_<double>(3, 1) << pt1.x, pt1.y, 1);       // y1=(X/Z, Y/Z, 1) 相机归一化坐标
Point2d pt2 = pixel2cam(keypoints_2[m.trainIdx].pt, K);
Mat y2 = (Mat_<double>(3, 1) << pt2.x, pt2.y, 1);
```

获得两个像素的归一化坐标之后，使用$x_2^{T}t^{\land}Rx_1=0$，进行验证：

```cpp
Mat d = y2.t() * t_x * R * y1; //  y2.t()表示y2的转置，t_x表示t^
cout << "epipolar constraint = " << d << endl;
```

这仅仅是一对像素点，最终我们会发现良好的匹配点一共有79对，因此需要写一个循环

```cpp
/**************
* 3.验证对极约束
**************/
Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
for (DMatch m : good_matches)
{
    Point2d pt1 = pixel2cam(keypoints_1[m.queryIdx].pt, K); // keypoints_1[m.queryIdx].pt为像素坐标，K为相机内参矩阵
    Mat y1 = (Mat_<double>(3, 1) << pt1.x, pt1.y, 1);       // y1=(X/Z, Y/Z, 1) 相机归一化坐标
    Point2d pt2 = pixel2cam(keypoints_2[m.trainIdx].pt, K);
    Mat y2 = (Mat_<double>(3, 1) << pt2.x, pt2.y, 1);
    Mat d = y2.t() * t_x * R * y1; //  y2.t()表示y2的转置，t_x表示t^
    cout << "epipolar constraint = " << d << endl;
}
```

## 整体代码

主函数，其中调用的方法，前文已经列出

```cpp
#include <iostream>
#include <opencv2/core/core.hpp>
#include <opencv2/features2d/features2d.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/calib3d/calib3d.hpp>

using namespace std;
using namespace cv;

int main(int argc, char **argv)
{
    if (argc != 3)
    {
        cout << "请输入相邻两帧的图像" << endl;
        return 1;
    }
    // 读入图像
    Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
    Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);

    std::vector<KeyPoint> keypoints_1, keypoints_2;
    vector<DMatch> good_matches;
    /**************
     * 1.特征匹配
     **************/
    find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, good_matches);
    cout << "一共找到了" << good_matches.size() << "对匹配点" << endl;
    /**************
     * 2.估计两张图像间运动
     **************/
    Mat R, t;
    pose_estimation_2d2d(keypoints_1, keypoints_2, good_matches, R, t);

    // 验证E=t^R*scale
    Mat t_x = (Mat_<double>(3, 3) << 0, -t.at<double>(2.0), t.at<double>(1, 0),
               t.at<double>(2, 0), 0, -t.at<double>(0, 0) - t.at<double>(1, 0), t.at<double>(0, 0), 0);
    cout << "t^R: \n"
         << t_x * R << endl;

    /**************
     * 3.验证对极约束
     **************/
    Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
    for (DMatch m : good_matches)
    {
        Point2d pt1 = pixel2cam(keypoints_1[m.queryIdx].pt, K); // keypoints_1[m.queryIdx].pt为像素坐标，K为相机内参矩阵
        Mat y1 = (Mat_<double>(3, 1) << pt1.x, pt1.y, 1);       // y1=(X/Z, Y/Z, 1) 相机归一化坐标
        Point2d pt2 = pixel2cam(keypoints_2[m.trainIdx].pt, K);
        Mat y2 = (Mat_<double>(3, 1) << pt2.x, pt2.y, 1);
        Mat d = y2.t() * t_x * R * y1; //  y2.t()表示y2的转置，t_x表示t^
        cout << "epipolar constraint = " << d << endl;
    }

    return 0;
}

```

CMakeLists.txt

```cmake
cmake_minimum_required( VERSION 2.8 )
project( pose_estimation_2d2d )

set( CMAKE_BUILD_TYPE "Release" )
set( CMAKE_CXX_FLAGS "-std=c++14 -O3" )

find_package( OpenCV 3.1 REQUIRED )

include_directories( 
    ${OpenCV_INCLUDE_DIRS} 
    "/usr/include/eigen3/"
)

# add_executable( pose_estimation_2d2d pose_estimation_2d2d.cpp extra.cpp ) # use this if in OpenCV2 
add_executable( pose_estimation_2d2d pose_estimation_2d2d.cpp )
target_link_libraries( pose_estimation_2d2d ${OpenCV_LIBS} )
```

### 打印输出

```c#
一共找到了79对匹配点
基础矩阵F：
[4.544437503937326e-06, 0.0001333855576988952, -0.01798499246457619;
 -0.0001275657012959839, 2.266794804637672e-05, -0.01416678429258694;
 0.01814994639952877, 0.004146055871509035, 1]
本质矩阵E：
[0.01097677480088526, 0.2483720528258777, 0.03167429207291153;
 -0.2088833206039177, 0.02908423961781584, -0.6744658838357441;
 0.008286777636447118, 0.6614041624098427, 0.01676523772725936]
单应矩阵H：
[0.9261214281175537, -0.1445322024509824, 33.26921085699328;
 0.04535424464910424, 0.9386696693816731, 8.570979966717406;
 -1.006197561051869e-05, -3.008140280167741e-05, 0.9999999999999999]
R: 
[0.9969387384754708, -0.05155574188737422, 0.05878058527591362;
 0.05000441581290405, 0.998368531736214, 0.02756507279306545;
 -0.06010582439453526, -0.02454140006844053, 0.9978902793175882]
t: 
[-0.9350802885437915;
 -0.03514646275858852;
 0.3526890700495534]
t^R: 
[-0.01552350379276751, -0.3512511256212342, -0.04479421342842761;
 0.2932931178326273, -0.04199386952278789, 0.9889111138164588;
 -0.992323587628662, 0.0236673579316946, 0.942925712677014]
epipolar constraint = [0.6796225931993222]
epipolar constraint = [1.041980822107436]
........
epipolar constraint = [1.014767775015567]
epipolar constraint = [1.257231546320262]
epipolar constraint = [1.000924219722595]
epipolar constraint = [0.8732996303495971]
```

# 使用三角测量

上文的对极约束求解相机运动存在尺度不确定性：

>由于 $E$ 本身具有尺度等价性，它分解得到的$ t, R $也有一个尺度等价性。而$ R ∈ SO(3) $自身具有约束，所以我们认为 $t $具有一个尺度。换言之，在分解过程中，对$ t $乘以任意非零常数，分解都是成立的。因此，我们通常把 $t $进行归一化，让它的长度等于 1。

这也是由于单目相机本身的缺点，在单目 SLAM 中，仅通过单张图像无法获得像素的深度信息，我们需要通过三角测量（Triangulation）（或三角化）的方法来估计地图点的深度。

![image-20220325110908224](image-20220325110908224.png)

按照对极几何中的定义，设 $x_1 , x_2 $为两个特征点的归一化坐标，满足：
$$
\begin{align}
s_1p_1&=KP,\\ s_2p_2&=K(RP+t) 
\end{align}
$$

已知：
$$
\begin{align}
x_1 = K^{-1}p_1 \\
x_2 = K^{-1}p_2
\end{align}
$$

将$p_1,p_2$换成$x_1,x_2$:
$$
\begin{align}
s_1Kx_1&=KP,\\ s_2Kx_2&=K(RP+t) 
\end{align}
$$
因此，$x_1,x_2$的关系满足：
$$
s_2x_2= s_1Rx_1+t
$$
我们已经知道了 $R, t$，想要求解的是两个特征点的深度 $s_1 , s_2$ 。当然这两个深度是可以分开求的，比方说先来看 $s_1$ 。如果我要算 $s_1 $，那么先对上式两侧左乘一个 $x^{\land}_2$
$$
s_2x^{\land}_2x_2= s_1x^{\land}_2Rx_1+x^{\land}_2t\\
s_1x^{\land}_2Rx_1+x^{\land}_2t=0 \tag{i}
$$
这是一个关于$s_1$的方程，可以求得$s_1$的值，于是，我们就得到了两个帧下的点的深度，确定了它们的空间坐标。当然，由于噪声的存在，我们估得的 $R, t$，不一定精确使式$(i)$为零，所以更常见的做法求**最小二乘解**,而不是零解。

同样的，三角测量是在特征匹配以及估计两张图像间的运动的基础上进行的。之前我们已经获得了良好的匹配点，并通过本质矩阵求得相机的位姿$R,t$，在此基础上进行三角测量。

三角测量的方法：

```cpp
void triangulation(
    const std::vector<KeyPoint> &keypoints_1,
    const std::vector<KeyPoint> &keypoints_2,
    const std::vector<DMatch> &matches,
    const Mat &R, const Mat &t,
    vector<Point3d> &points)
{
    // 初始化单位阵(参考位姿矩阵)
    Mat T1 =
        (Mat_<float>(3, 4) << 
         1, 0, 0, 0,
         0, 1, 0, 0,
         0, 0, 1, 0);
    // 对极约束求出的变换矩阵
    Mat T2 =
        (Mat_<float>(3, 4) << 
         R.at<double>(0, 0), R.at<double>(0, 1), R.at<double>(0, 2), t.at<double>(0, 0),
         R.at<double>(1, 0), R.at<double>(1, 1), R.at<double>(1, 2), t.at<double>(1, 0),
         R.at<double>(2, 0), R.at<double>(2, 1), R.at<double>(2, 2), t.at<double>(2, 0));
    // 相机内参
    Mat K =
        (Mat_<double>(3, 3) << 
         520.9, 0,      325.1,
         0,     521.0,  249.7,
         0,     0,      1);
    vector<Point2f> pts_1, pts_2;
    // 遍历DMatch中匹配的特征点
    for (DMatch m : matches)
    {
        // 将像素坐标转换至归一化成像平面
        pts_1.push_back( pixel2cam(keypoints_1[m.queryIdx].pt,K) );
        pts_2.push_back( pixel2cam(keypoints_2[m.trainIdx].pt,K) );
    }
    
    // 调用三角化函数
    Mat pts_4d;
    //函数接受的参数是两个相机位姿和特征点在两个相机坐标系下的归一化坐标，输出三角化后的特征点的3D坐标。
    //输出的3D坐标是齐次坐标，共四个维度（所以变量名为pts_4d），因此需要将前三个维度除以第四个维度以得到非齐次坐标xyz
    cv::triangulatePoints(T1, T2, pts_1, pts_2, pts_4d);

    // 转换成非齐次坐标
    for (int i = 0; i < pts_4d.cols; i++)
    {
        Mat x = pts_4d.col(i);
        x /= x.at<float>(3,0);// 归一化
        Point3d p(x.at<float>(0,0), x.at<float>(1,0), x.at<float>(2,0));
        points.push_back(p);
    }
}
```

主函数

```cpp
int main(int argc, char **argv)
{
    if (argc != 3)
    {
        cout << "请输入相邻两帧的图片！" << endl;
        return 1;
    }
    // 读入图像
    Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
    Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);

    // 1.特征提取与匹配
    vector<KeyPoint> keypoints_1, keypoints_2;
    vector<DMatch> matches;
    find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);
    cout << "一共找到了" << matches.size() << "个良好的匹配点" << endl;

    // // 2.估计两张图像间运动
    Mat R, t;
    pose_estimation_2d2d(keypoints_1, keypoints_2, matches, R, t);

    // 3.三角化
    vector<Point3d> points;
    triangulation(keypoints_1,keypoints_2,matches,R,t,points);

    // 4.验证三角化点与特征点的重投影关系
    Mat K = ( Mat_<double> ( 3,3 ) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 );
    for (int i = 0; i < matches.size(); i++)
    {
        cout << "--------第["<< i+1 <<"]个空间点-----------" << endl;
		
        // 第一个图
        Point2d pt1_cam = pixel2cam(keypoints_1[matches[i].queryIdx].pt, K);
        Point2d pt1_cam_3d(points[i].x / points[i].z, points[i].y / points[i].z); // 归一化
        cout << "点在第一帧时的像素坐标: " << pt1_cam << endl;
        cout << "点在第一帧时3D投影坐标：" << pt1_cam_3d << endl;
        cout << "深度d:" << points[i].z << endl;
        // 第二个图
        Point2f pt2_cam = pixel2cam(keypoints_2[matches[i].trainIdx].pt, K);
        Mat pt2_trans = R*(Mat_<double>(3,1) << points[i].x, points[i].y, points[i].z) + t;
        pt2_trans /= pt2_trans.at<double>(2,0); // 归一化
        cout << "点在第二帧时的像素坐标: " << pt2_cam << endl;
        cout << "点在第二帧时3D投影坐标：" << pt2_trans.t() << endl; 
        
    }
    return 0;
}
```

打印输出：

```c#
-- Max dist : 94.000000 
-- Min dist : 4.000000 
一共找到了79个良好的匹配点
基础矩阵F：
[4.544437503937326e-06, 0.0001333855576988952, -0.01798499246457619;
 -0.0001275657012959839, 2.266794804637672e-05, -0.01416678429258694;
 0.01814994639952877, 0.004146055871509035, 1]
本质矩阵E：
[0.01097677480088526, 0.2483720528258777, 0.03167429207291153;
 -0.2088833206039177, 0.02908423961781584, -0.6744658838357441;
 0.008286777636447118, 0.6614041624098427, 0.01676523772725936]
单应矩阵H：
[0.9261214281175537, -0.1445322024509824, 33.26921085699328;
 0.04535424464910424, 0.9386696693816731, 8.570979966717406;
 -1.006197561051869e-05, -3.008140280167741e-05, 0.9999999999999999]
R: 
[0.9969387384754708, -0.05155574188737422, 0.05878058527591362;
 0.05000441581290405, 0.998368531736214, 0.02756507279306545;
 -0.06010582439453526, -0.02454140006844053, 0.9978902793175882]
t: 
[-0.9350802885437915;
 -0.03514646275858852;
 0.3526890700495534]
--------第[1]个空间点-----------
点在第一帧时的像素坐标: [-0.0136303, -0.302687]
点在第一帧时3D投影坐标：[-0.0136588, -0.302975]
深度d:14.4036
点在第二帧时的像素坐标: [-0.00403148, -0.270058]
点在第二帧时3D投影坐标：[-0.004013293769641682, -0.2697748885145957, 1]
--------第[2]个空间点-----------
点在第一帧时的像素坐标: [-0.153772, -0.0742802]
点在第一帧时3D投影坐标：[-0.153809, -0.0755175]
深度d:9.63635
点在第二帧时的像素坐标: [-0.179497, -0.0577735]
点在第二帧时3D投影坐标：[-0.1795174494211173, -0.05658791842651693, 1]
--------第[3]个空间点-----------
点在第一帧时的像素坐标: [-0.468612, 0.119578]
点在第一帧时3D投影坐标：[-0.46862, 0.119797]
深度d:7.88393
点在第二帧时的像素坐标: [-0.499328, 0.1119]
点在第二帧时3D投影坐标：[-0.4993080966028579, 0.1116965292982101, 1]
--------第[4]个空间点-----------
点在第一帧时的像素坐标: [-0.226723, 0.0735125]
点在第一帧时3D投影坐标：[-0.226723, 0.0735073]
深度d:8.35771
点在第二帧时的像素坐标: [-0.268958, 0.08119]
点在第二帧时3D投影坐标：[-0.2689579480646592, 0.0811949362011327, 1]
.......................(略)
```

我们打印了每个空间点在两个相机坐标系下的投影坐标与像素坐标——相当于 P 的投影位置与看到的特征点位置。由于误差的存在，它们会有一些微小的差异。可以看到，误差的量级大约在小数点后第三位。可以看到，三角化特征点的距离大约为 14～15。但由于尺度不确定性，我们并不知道这里的 15 究竟是多少米。

# 拓展

在ORBSLAM中这三种方法的顺序：

- 使用**特征点检测和特征匹配**，寻找比较好的匹配点。

- 由两张图像的匹配点，利用**对极几何**计算出$H$或者$F$矩阵，并从这两个矩阵中恢复出$R，t$

- 有了$R,t $就可以利用相机的位姿和两帧对应的像素坐标用**三角测量**计算出其对应的3D点坐标。至此，相机的位姿和对应的地图点就都有了，接下来正常跟踪即可；

- 跟踪丢失后，就需要回到原来机器人曾经经过的位置找匹配帧，找到的匹配帧是有其3D地图点和位姿的，用这些3D点和当前帧自己的像素坐标， PnP计算出当前帧相较于匹配帧的运动$R，t$

  > 参考：https://blog.csdn.net/qq_25458977/article/details/113675519

